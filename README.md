# ğŸ“ˆ Stock Data Pipeline Project

A comprehensive real-time stock data pipeline built with Apache Kafka, Apache , and Streamlit for data processing, analysis, and visualization.

## ğŸ¯ Overview

This project implements a complete ETL pipeline for stock market data that:
- **Extracts** real-time stock data from multiple APIs
- **Transforms** and cleans the data using pandas and
- **Loads** processed data into Google Cloud Storage and PostgreSQL
- **Visualizes** insights through an interactive Streamlit dashboard
- **Streams** real-time data using Apache Kafka

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
â”‚   Data Sources  â”‚    â”‚   Apache Kafka  â”‚    
â”‚                 â”‚    â”‚                 â”‚    
â”‚ â€¢ Alpha Vantage â”‚â”€â”€â”€â–¶â”‚ â€¢ Stock Prices â”‚â”€â”€â”€-----------|
â”‚ â€¢ Yahoo Finance â”‚    â”‚ â€¢ News Feeds    â”‚              |
â”‚ â€¢ News APIs     â”‚    â”‚ â€¢ Market Events â”‚              |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              |
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   Streamlit     â”‚    â”‚   Data Storage  â”‚              â”‚
â”‚   Dashboard     â”‚    â”‚                 â”‚              â”‚
â”‚                 â”‚â—€â”€â”€â”€â”‚ â€¢ PostgreSQL    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Real-time viz â”‚    â”‚ â€¢ Google Cloud  â”‚
â”‚ â€¢ Analytics     â”‚    â”‚   Storage       â”‚
â”‚ â€¢ Monitoring    â”‚    â”‚ â€¢ Local Files   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

Before running this project, ensure you have:

- **Python 3.8+** installed
- **Docker** and **Docker Compose** installed
- **Google Cloud Platform** account with service account key
- **API Keys** for:
  - Alpha Vantage (free at [alphavantage.co](https://www.alphavantage.co/support/#api-key))
  - News API (free at [newsapi.org](https://newsapi.org/register))

### 1. Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd stock-data-pipeline-project

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env file with your credentials
nano .env
```

**Required Environment Variables:**
```env
# API Keys
ALPHA_VANTAGE_API_KEY=your_alpha_vantage_key
NEWS_API_KEY=your_news_api_key

# Database
POSTGRES_USER=stockdb_user
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=stockdb



# Google Cloud
GOOGLE_APPLICATION_CREDENTIALS=secrets/gcs-key.json
GCS_BUCKET_NAME=your-gcs-bucket
```

### 3. Google Cloud Setup

```bash
# Create secrets directory
mkdir -p secrets

# Add your GCS service account key
cp /path/to/your/gcs-key.json secrets/gcs-key.json

# Copy configuration file
cp config.yaml secrets/config.yaml
```

### 4. Start the Pipeline

```bash
# Start all services with Docker Compose
docker-compose up -d

# Or start specific services
docker-compose up kafka zookeeper postgres -d
```

### 5. Access Applications

- **Streamlit Dashboard**: http://localhost:8501
- ** Web UI**: http://localhost:8080
- **Kafka UI**: http://localhost:9021 (if enabled)

## ğŸ“ Project Structure

```
stock-data-pipeline-project/
â”œâ”€â”€ ğŸ“ /                 # Apache  DAGs and plugins
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ stock_data_dag.py   # Main ETL pipeline DAG
â”‚   â”‚   â””â”€â”€ news_data_dag.py    # News data collection DAG
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ ğŸ“ data_collector/          # Data extraction modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ alpha_vantage.py        # Alpha Vantage API client
â”‚   â”œâ”€â”€ yahoo_finance.py        # Yahoo Finance API client
â”‚   â””â”€â”€ news_collector.py       # News data collector
â”œâ”€â”€ ğŸ“ model/                   # Data models and transformations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ stock_model.py          # Stock data models
â”‚   â””â”€â”€ transformations.py      # Data transformation functions
â”œâ”€â”€ ğŸ“ gcs_uploader/           # Google Cloud Storage utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ uploader.py            # GCS upload functions
â”œâ”€â”€ ğŸ“ streamlit/              # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py                 # Main dashboard app
â”‚   â”œâ”€â”€ pages/                 # Multi-page dashboard
â”‚   â””â”€â”€ components/            # Reusable UI components
â”œâ”€â”€ ğŸ“ my_kafka/               # Kafka producers and consumers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ producer.py            # Kafka data producer
â”‚   â””â”€â”€ consumer.py            # Kafka data consumer
â”œâ”€â”€ ğŸ“ docker/                 # Docker configurations
â”‚   â””â”€â”€ /               #  Docker setup
â”œâ”€â”€ ğŸ“„ docker-compose.yml      # Docker services configuration
â”œâ”€â”€ ğŸ“„ requirements.txt        # Python dependencies
â”œâ”€â”€ ğŸ“„ config.yaml            # Application configuration
â”œâ”€â”€ ğŸ“„ .env.example           # Environment variables template
â””â”€â”€ ğŸ“„ README.md              # This file
```

## ğŸ”§ Development Setup

### Local Development

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run pre-commit hooks
pre-commit install

# Run tests
python -m pytest tests/

# Start Streamlit for development
streamlit run streamlit/app.py

# Start Kafka locally (optional)
cd my_kafka
python producer.py
```

### Database Setup

```bash
# Initialize PostgreSQL database
python scripts/init_db.py

# Run migrations
python scripts/migrate.py

# Seed with sample data
python scripts/seed_data.py
```

## ğŸ“Š Features

### Real-time Data Pipeline
- **Multi-source data ingestion** from Alpha Vantage, Yahoo Finance, and news APIs
- **Apache Kafka** for real-time data streaming
- **Apache ** for workflow orchestration and scheduling
- **Automated data quality checks** and error handling

### Data Storage & Processing
- **PostgreSQL** for structured data storage
- **Google Cloud Storage** for data lake functionality
- **Pandas-based transformations** for data cleaning and feature engineering
- **Configurable data retention** policies

### Interactive Dashboard
- **Real-time stock price monitoring** with live updates
- **Technical analysis indicators** (SMA, EMA, RSI, MACD)
- **News sentiment analysis** integrated with stock data
- **Portfolio tracking** and performance analytics
- **Customizable alerts** and notifications

### DevOps & Monitoring
- **Docker containerization** for easy deployment
- **Health checks** and service monitoring
- **Centralized logging** with structured logs
- **Environment-based configuration** management

## ğŸ› ï¸ Configuration

### Main Configuration (config.yaml)

```yaml
# Data Sources
data_sources:
  alpha_vantage:
    base_url: "https://www.alphavantage.co/query"
    rate_limit: 5  # requests per minute
  
  news_api:
    base_url: "https://newsapi.org/v2"
    rate_limit: 100  # requests per day

# Database
database:
  host: "postgres"
  port: 5432
  name: "stockdb"
  
# Kafka
kafka:
  bootstrap_servers: ["kafka:9092"]
  topics:
    stock_prices: "stock-prices"
    news_feed: "news-feed"

# GCS
gcs:
  bucket_name: "your-stock-data-bucket"
  credentials_path: "secrets/gcs-key.json"
```

###  DAGs Configuration

The project includes several pre-configured DAGs:

- **`stock_data_dag.py`**: Main ETL pipeline (runs every 15 minutes)
- **`news_data_dag.py`**: News data collection (runs hourly)
- **`data_quality_dag.py`**: Data quality checks (runs daily)

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest

# Run specific test categories
python -m pytest tests/unit/          # Unit tests
python -m pytest tests/integration/   # Integration tests
python -m pytest tests/e2e/          # End-to-end tests

# Run with coverage
python -m pytest --cov=src --cov-report=html
```

## ğŸ“ˆ Monitoring & Observability

### Health Checks

All services include health check endpoints:

```bash
# Check service health
curl http://localhost:8501/health      # Streamlit
curl http://localhost:8080/health      # 
```

### Logging

Structured logging is configured for all components:

```bash
# View logs
docker-compose logs -f streamlit
docker-compose logs -f -webserver
```

### Metrics

Key metrics are tracked:
- Data ingestion rates
- Processing latencies  
- Error rates
- Data quality scores

## ğŸš€ Deployment

### Development Deployment

```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f
```

### Production Deployment

For production deployment, refer to the production checklist:

1. **Security**: Update all default passwords and API keys
2. **SSL/TLS**: Configure HTTPS with proper certificates  
3. **Monitoring**: Set up Prometheus and Grafana
4. **Backup**: Configure automated database backups
5. **Scaling**: Adjust resource limits and replica counts

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines

- Follow PEP 8 style guidelines
- Write comprehensive tests for new features
- Update documentation for API changes
- Use type hints where applicable

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Apache Software Foundation** for Kafka and 
- **Streamlit** for the amazing dashboard framework
- **Alpha Vantage** and **Yahoo Finance** for stock data APIs
- **Google Cloud Platform** for storage and compute services



**Happy Trading! ğŸ“ˆğŸ’°**
